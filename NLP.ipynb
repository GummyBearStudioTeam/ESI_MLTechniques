{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP(5).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "9WLOQHuQh7mX",
        "3ZdP2WaTjTIm",
        "aZ3loh-2YyXK"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3BBCJdHQGOx"
      },
      "source": [
        "# NLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kENXF_vPOrD7",
        "outputId": "51a00ff6-669e-4d28-e0ad-aab18ea9b4d1"
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk.corpus\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import seaborn as sns\n",
        "\n",
        "from math import ceil\n",
        "\n",
        "from sklearn import metrics"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NazHONfiVqED"
      },
      "source": [
        "# 1. Preprocessing\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DUSxKRnOU3y"
      },
      "source": [
        "## Loading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9l7NB9VLh1s"
      },
      "source": [
        "df = pd.read_csv(\"labeled_data.csv\")\n",
        "df.drop(df.columns[0], inplace=True, axis=1)\n",
        "hatred_dict = pd.read_csv(\"refined_ngram_dict.csv\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        },
        "id": "44Th0QTQ90sL",
        "outputId": "0f63951e-a79b-4f26-9a50-47f26552c0eb"
      },
      "source": [
        "df"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>hate_speech</th>\n",
              "      <th>offensive_language</th>\n",
              "      <th>neither</th>\n",
              "      <th>class</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24778</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>you's a muthaf***in lie &amp;#8220;@LifeAsKing: @2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24779</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>you've gone and broke the wrong heart baby, an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24780</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>young buck wanna eat!!.. dat nigguh like I ain...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24781</th>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>youu got wild bitches tellin you lies</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24782</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>~~Ruffled | Ntac Eileen Dahlia - Beautiful col...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>24783 rows Ã— 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       count  ...                                              tweet\n",
              "0          3  ...  !!! RT @mayasolovely: As a woman you shouldn't...\n",
              "1          3  ...  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...\n",
              "2          3  ...  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...\n",
              "3          3  ...  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...\n",
              "4          6  ...  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...\n",
              "...      ...  ...                                                ...\n",
              "24778      3  ...  you's a muthaf***in lie &#8220;@LifeAsKing: @2...\n",
              "24779      3  ...  you've gone and broke the wrong heart baby, an...\n",
              "24780      3  ...  young buck wanna eat!!.. dat nigguh like I ain...\n",
              "24781      6  ...              youu got wild bitches tellin you lies\n",
              "24782      3  ...  ~~Ruffled | Ntac Eileen Dahlia - Beautiful col...\n",
              "\n",
              "[24783 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "eJRrRp6lQGPD",
        "outputId": "5d4731f4-bd19-4480-bd8b-7fb7939b883b"
      },
      "source": [
        "hatred_dict"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ngram</th>\n",
              "      <th>prophate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>allah akbar</td>\n",
              "      <td>0.870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>blacks</td>\n",
              "      <td>0.583</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>chink</td>\n",
              "      <td>0.467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>chinks</td>\n",
              "      <td>0.542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>dykes</td>\n",
              "      <td>0.602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>nigga you a lame</td>\n",
              "      <td>0.556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>niggers are in my</td>\n",
              "      <td>0.714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>wit a lame nigga</td>\n",
              "      <td>0.556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>you a lame bitch</td>\n",
              "      <td>0.556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>you fuck wit a</td>\n",
              "      <td>0.556</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>178 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 ngram  prophate\n",
              "0          allah akbar     0.870\n",
              "1               blacks     0.583\n",
              "2                chink     0.467\n",
              "3               chinks     0.542\n",
              "4                dykes     0.602\n",
              "..                 ...       ...\n",
              "173   nigga you a lame     0.556\n",
              "174  niggers are in my     0.714\n",
              "175   wit a lame nigga     0.556\n",
              "176   you a lame bitch     0.556\n",
              "177     you fuck wit a     0.556\n",
              "\n",
              "[178 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fObzoThITjB3",
        "outputId": "18fb7a9d-0398-4d75-a2b2-be305221af3e"
      },
      "source": [
        "df.isna().sum()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count                 0\n",
              "hate_speech           0\n",
              "offensive_language    0\n",
              "neither               0\n",
              "class                 0\n",
              "tweet                 0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "CdsNfCLD94QD",
        "outputId": "6e10033d-5c0e-42c7-a0fb-5b35ffeb4e80"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>hate_speech</th>\n",
              "      <th>offensive_language</th>\n",
              "      <th>neither</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>24783.000000</td>\n",
              "      <td>24783.000000</td>\n",
              "      <td>24783.000000</td>\n",
              "      <td>24783.000000</td>\n",
              "      <td>24783.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>3.243473</td>\n",
              "      <td>0.280515</td>\n",
              "      <td>2.413711</td>\n",
              "      <td>0.549247</td>\n",
              "      <td>1.110277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.883060</td>\n",
              "      <td>0.631851</td>\n",
              "      <td>1.399459</td>\n",
              "      <td>1.113299</td>\n",
              "      <td>0.462089</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>9.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              count   hate_speech  ...       neither         class\n",
              "count  24783.000000  24783.000000  ...  24783.000000  24783.000000\n",
              "mean       3.243473      0.280515  ...      0.549247      1.110277\n",
              "std        0.883060      0.631851  ...      1.113299      0.462089\n",
              "min        3.000000      0.000000  ...      0.000000      0.000000\n",
              "25%        3.000000      0.000000  ...      0.000000      1.000000\n",
              "50%        3.000000      0.000000  ...      0.000000      1.000000\n",
              "75%        3.000000      0.000000  ...      0.000000      1.000000\n",
              "max        9.000000      7.000000  ...      9.000000      2.000000\n",
              "\n",
              "[8 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_BdK0xmXI9d"
      },
      "source": [
        "## Preparing the data to preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7BmLnslXIJp"
      },
      "source": [
        "data_set = pd.DataFrame()\n",
        "data_set['clean'] = df['tweet']"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVIKSahQWo4U"
      },
      "source": [
        "### Limiting processed data\n",
        "To reduce memory usage a data limit can be applied."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dG2J5IG_WlaA"
      },
      "source": [
        "N = None"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbRzbg6IWn-P"
      },
      "source": [
        "if N is not None:\n",
        "    data_set = data_set[0:N]\n",
        "    df = df[0:N]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1AJs3MehsYg"
      },
      "source": [
        "## Correction dictionaries - to hide"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WLOQHuQh7mX"
      },
      "source": [
        "### Contractions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bp1lbiujhm6V"
      },
      "source": [
        "contractions = { \n",
        "\"ain't\": \"am not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he'll've\": \"he will have\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'd'y\": \"how do you\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how is\",\n",
        "\"I'd\": \"I would\",\n",
        "\"I'd've\": \"I would have\",\n",
        "\"I'll\": \"I will\",\n",
        "\"I'll've\": \"I will have\",\n",
        "\"I'm\": \"I am\",\n",
        "\"I've\": \"I have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'd've\": \"it would have\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it'll've\": \"it will have\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"mightn't've\": \"might not have\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"mustn't've\": \"must not have\",\n",
        "\"needn't\": \"need not\",\n",
        "\"needn't've\": \"need not have\",\n",
        "\"o'clock\": \"of the clock\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"oughtn't've\": \"ought not have\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"shan't've\": \"shall not have\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'd've\": \"she would have\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she'll've\": \"she will have\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"shouldn't've\": \"should not have\",\n",
        "\"so've\": \"so have\",\n",
        "\"so's\": \"so as\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that'd've\": \"that would have\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there would\",\n",
        "\"there'd've\": \"there would have\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"they would\",\n",
        "\"they'd've\": \"they would have\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they'll've\": \"they will have\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"to've\": \"to have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we would\",\n",
        "\"we'd've\": \"we would have\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we'll've\": \"we will have\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what will\",\n",
        "\"what'll've\": \"what will have\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"when's\": \"when is\",\n",
        "\"when've\": \"when have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where is\",\n",
        "\"where've\": \"where have\",\n",
        "\"who'll\": \"who will\",\n",
        "\"who'll've\": \"who will have\",\n",
        "\"who's\": \"who is\",\n",
        "\"who've\": \"who have\",\n",
        "\"why's\": \"why is\",\n",
        "\"why've\": \"why have\",\n",
        "\"will've\": \"will have\",\n",
        "\"won't\": \"will not\",\n",
        "\"won't've\": \"will not have\",\n",
        "\"would've\": \"would have\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"wouldn't've\": \"would not have\",\n",
        "\"y'all\": \"you all\",\n",
        "\"y'all'd\": \"you all would\",\n",
        "\"y'all'd've\": \"you all would have\",\n",
        "\"y'all're\": \"you all are\",\n",
        "\"y'all've\": \"you all have\",\n",
        "\"you'd\": \"you would\",\n",
        "\"you'd've\": \"you would have\",\n",
        "\"you'll\": \"you will\",\n",
        "\"you'll've\": \"you will have\",\n",
        "\"you're\": \"you are\",\n",
        "\"you've\": \"you have\"\n",
        "}"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZdP2WaTjTIm"
      },
      "source": [
        "### Others"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fV2XR4mBjSNc"
      },
      "source": [
        "adjustments = {\n",
        "    'ya': 'you',\n",
        "    'aw': '',\n",
        "    'dat': 'that',\n",
        "    'dem': 'them',\n",
        "    'll': 'will',\n",
        "    'u': 'you'\n",
        "}\n",
        "\n",
        "abbreviations = {\n",
        "    'idk': 'I do not know',\n",
        "    'btw': 'by the way',\n",
        "    'pls': 'please'\n",
        "}\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZ3loh-2YyXK"
      },
      "source": [
        "### Emoticons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEBQS--eYwxS"
      },
      "source": [
        "emoticons = {\n",
        "    '&#8220': '', #\"\n",
        "    '&#128555': 'tired',\n",
        "    '&#128553': 'weary',\n",
        "    '&#128557': 'crying',\n",
        "    '&#128514': 'joy',\n",
        "    '&#128131': '',\n",
        "    '&#128149': 'love',\n",
        "    '&#128095': '',\n",
        "    '&#128128': 'death',\n",
        "    '&#127813': '',\n",
        "    '&#127829': '',\n",
        "    '&#128064': '',\n",
        "    '&#128073': '',\n",
        "    '&#128077': 'ok',\n",
        "    '&#127867': '',\n",
        "    '&#9733': '',\n",
        "    '&#127942': '',\n",
        "    '&#128034': '',\n",
        "    '&#128072': '',\n",
        "    '&#128075': '',\n",
        "    '&#128530': 'unamused',\n",
        "    '&#128563': '', #hands up\n",
        "    '&#128175': '',\n",
        "    '&#128588': '', #hands up\n",
        "}"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysMH1zIvoz9y",
        "outputId": "57306891-5ff5-444a-c953-98be74793946"
      },
      "source": [
        "i = 0\n",
        "for tweet in df['tweet']:\n",
        "  if re.match('&#\\d{4}', tweet):\n",
        "    if any(True for emot in emoticons if emot in tweet):\n",
        "      continue\n",
        "    print(i, tweet)\n",
        "    i += 1"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 &#128079; congrats you've turned a hoe into a housewife, don't get shitty when your guys start singing they hit it first. #ButThatsNoneOfMyBusiness\n",
            "1 &#128165;&#128162; on the pussy http://t.co/mWXQnjm4So\n",
            "2 &#128347; is the most important thing. All this temporary bullshit and lies is fa the birds. Kill that !\n",
            "3 &#128520;&#127383; we snap chatted for one night lol. But you're cute. Snapchat me back nig\n",
            "4 &#128527; haahaa ,dumb bitch\n",
            "5 &#128532; RT @MichyDoe: Every week them hoes partying ! I see them hoes in every city partying for an event\n",
            "6 &#128539;&#128120; you've been a good as friend to me , glad I got your honkie ass. Let's fuck shit up this year\n",
            "7 &#128540;&#128583;&#128582; I hate ya bitch ass\n",
            "8 &#128583;&#128583;&#128583;&#128583; can y'all females let this sink in for min? Moment of silence to get y'all bitches thinking right? http://t.co/cDN0dEYCkO\n",
            "9 &#128700;&#128700;- you a little genius man i need you for a class so u can help a nig out but you cool af man\n",
            "10 &#8216;Chillin&#8217; With My Homie Or What&#8217;s Left Of Him&#8217;: British Rapper Turned ISIS Jihadi Poses With Severed Head http://t.co/L9vMdNOXPg #tcot\n",
            "11 &#9889;&#65039;&#9889;&#65039; why lie you're my nigguh &#128526;\n",
            "12 &#9996;&#65039; out bitch I'm winning &#128526; http://t.co/k3l15zoBGz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCrAY6mOOdOM"
      },
      "source": [
        "## Correcting words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28ygMggGOPdp"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "def get_words(text):\n",
        "    return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "def replace_jargon(word):\n",
        "    for jargon_dict in [contractions, abbreviations, adjustments]:\n",
        "        if word in jargon_dict:\n",
        "          return jargon_dict[word]\n",
        "    return word\n",
        "\n",
        "def replace_jargon_from_array(arr):\n",
        "    return [replace_jargon(w) for w in arr]\n",
        "\n",
        "def replace_jargon_from_text(text):\n",
        "    words = get_words(text)\n",
        "    return replace_jargon_from_array(words)\n",
        "\n",
        "def delete_multiplied_letters(arr):\n",
        "  changed = []\n",
        "  for word in arr:\n",
        "      word = re.sub(r'(\\w)\\1{2,}', r'\\1', word)\n",
        "      changed.append(word)\n",
        "  return changed\n",
        "\n",
        "\n",
        "WORDS = Counter(get_words(open('big.txt').read()))\n",
        "\n",
        "#TO IMPROVE?\n",
        "def correct_array(words):\n",
        "   return [correction(replace_jargon(w)) for w in words if w not in WORDS]\n",
        "\n",
        "\n",
        "def correct_text(text):\n",
        "    words = get_words(text)\n",
        "    return correct_array(words)\n",
        "\n",
        "def P(word, N=sum(WORDS.values())): \n",
        "    \"Probability of `word`.\"\n",
        "    return WORDS[word] / N\n",
        "\n",
        "def correction(word): \n",
        "    \"Most probable spelling correction for word.\"\n",
        "    best = max(candidates(word), key=P)\n",
        "    if best == 'a':\n",
        "      print(word, '=>', best)\n",
        "    return best\n",
        "\n",
        "def candidates(word): \n",
        "    \"Generate possible spelling corrections for word.\"\n",
        "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "\n",
        "def known(words): \n",
        "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "    return set(w for w in words if w in WORDS)\n",
        "\n",
        "def edits1(word):\n",
        "    \"All edits that are one edit away from `word`.\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "  \n",
        "def edits2(word): \n",
        "    \"All edits that are two edits away from `word`.\"\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
        "\n",
        "## Lemmatizing\n",
        "\n",
        "def lemmatize_array(arr):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return [lemmatizer.lemmatize(word) for word in arr]\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    return lemmatize_array(get_words(text))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GnqAk20H47d"
      },
      "source": [
        "## Cleaning tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhB7nhDMH8vl"
      },
      "source": [
        "special_characters_regex = '[!\"_$%&/()=_Ë†*Â¡@' ',:;?#]'\n",
        "retweet_regex = '(.*rt @\\w+)+:'\n",
        "space_regex = '\\s+'\n",
        "url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
        "             '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
        "emoticon_regex = '&#\\d+;'\n",
        "mention_regex = '@[\\w\\-]+'\n",
        "number_regex = '\\d+'"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYQVczlUQGPN"
      },
      "source": [
        "data_set['clean'] = data_set.apply(lambda row:\n",
        "                        re.sub(space_regex, ' ',\n",
        "                        re.sub(special_characters_regex, '', \n",
        "                        re.sub(number_regex, ' NUMBERHERE ',\n",
        "                        re.sub('\\s*RT MENTIONHERE', ' MENTIONHERE ',\n",
        "                        re.sub(url_regex, ' LINKHERE ',\n",
        "                        re.sub(mention_regex, ' MENTIONHERE ',\n",
        "                        re.sub(retweet_regex, '',\n",
        "                        re.sub(space_regex, ' ',\n",
        "                              row['clean'])))))), flags=re.ASCII)), axis=1)\n",
        "\n",
        "data_set['clean'] = data_set.apply(lambda row: row['clean'].lower(), axis=1)\n",
        "data_set['clean'] = data_set.apply(lambda row: replace_jargon_from_text(row['clean']), axis=1)\n",
        "data_set['clean'] = data_set.apply(lambda row: delete_multiplied_letters(row['clean']), axis=1)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5xB2vwFRkyX"
      },
      "source": [
        "## Word Correction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNoqm2rEaER5",
        "outputId": "059c67b3-4f89-49ce-91fc-24540b947e0a"
      },
      "source": [
        "data_set['clean']"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        [rt, mentionhere, as, a, woman, you, shouldn, ...\n",
              "1        [rt, mentionhere, boy, dats, cold, tyga, dwn, ...\n",
              "2        [rt, mentionhere, dawg, rt, mentionhere, you, ...\n",
              "3        [rt, mentionhere, mentionhere, she, look, like...\n",
              "4        [rt, mentionhere, the, shit, you, hear, about,...\n",
              "                               ...                        \n",
              "24778    [you, s, a, muthafin, lie, numberhere, mention...\n",
              "24779    [you, ve, gone, and, broke, the, wrong, heart,...\n",
              "24780    [young, buck, wanna, eat, that, nigguh, like, ...\n",
              "24781        [youu, got, wild, bitches, tellin, you, lies]\n",
              "24782    [ruffled, ntac, eileen, dahlia, beautiful, col...\n",
              "Name: clean, Length: 24783, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMABvmnBRkAX"
      },
      "source": [
        "data_set['clean'] = data_set.apply(lambda row: correct_array(row['clean']), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HpQbw0DOcj9"
      },
      "source": [
        "data_set['clean'][:30]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2geb8EiQGPO"
      },
      "source": [
        "#data_set['clean'][24778]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0m-Qatsm7cHo"
      },
      "source": [
        "## Lemmatizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5lyy-vN7ZDX"
      },
      "source": [
        "data_set['clean'] = data_set.apply(lambda row: lemmatize_array(row['clean']), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpmD254bZeSi"
      },
      "source": [
        "data_set['clean'][:20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPfeavePQGPS"
      },
      "source": [
        "### Setup working sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeof13oPxGO5"
      },
      "source": [
        "y = df[['class']]\n",
        "X = pd.DataFrame() #df[['hate_speech', 'offensive_language', 'neither']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1v3o8k_LcJi"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AOni-T8LEBq"
      },
      "source": [
        "sentences = data_set.apply(lambda row: sent_tokenize(' '.join(row['clean'])),axis=1)\n",
        "words = data_set.apply(lambda row: word_tokenize(' '.join(row['clean'])), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKKo7WxjOGI-"
      },
      "source": [
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "stopwords += [',', '.', ';']\n",
        "data_set['clean'] = words.apply(lambda row: [w for w in row if w not in stopwords]) \n",
        "data_set['clean']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25O89_lgWAHz"
      },
      "source": [
        "# 2. Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmK81KSN8h0b"
      },
      "source": [
        "joined = data_set.apply(lambda row: ' '.join(row['clean']), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZukT4a0nQGPh"
      },
      "source": [
        "joined"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lySCoviQHBvv"
      },
      "source": [
        "### TFiDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMVh1GlfFXbI"
      },
      "source": [
        "vectorizer = TfidfVectorizer(min_df=1)\n",
        "X_tf = vectorizer.fit_transform(joined)\n",
        "X_tf.column = vectorizer.get_feature_names()\n",
        "X_tf.toarray()[0]\n",
        "#X.column\n",
        "X_tf.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g47n-vHbQGPi"
      },
      "source": [
        "X_tf.column"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Sbad0IWHGGP"
      },
      "source": [
        "### TFiDF + N-grams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZwZLemXDJJk"
      },
      "source": [
        "vectorizer = TfidfVectorizer(ngram_range=(2,3), min_df=1)\n",
        "X_ngram = vectorizer.fit_transform(joined)\n",
        "X_ngram.column = vectorizer.get_feature_names()\n",
        "X_ngram.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_da_pPHQGPn"
      },
      "source": [
        "X_ngram.sum(axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnlnj_DSHIsT"
      },
      "source": [
        "### TFiDF + N-grams + POS tagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MdhGYTFfAtg"
      },
      "source": [
        "tagged = data_set['clean'].apply(nltk.pos_tag)\n",
        "tagged"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8C9Ap1rsHNa6"
      },
      "source": [
        "### Other Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jTAeVNEJXI6"
      },
      "source": [
        "#### RTs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayPzlxtRJWqS"
      },
      "source": [
        "X['RT'] = df.apply(lambda row: row[\"tweet\"].count(\"RT\") , axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsEHbtXIMzVI"
      },
      "source": [
        "#### Number of words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7JA26kaMykj"
      },
      "source": [
        "X['num_words'] = words.apply(len)\n",
        "X.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7bU2ZJSNXHg"
      },
      "source": [
        "#### Number of sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bay_BUQNZdP"
      },
      "source": [
        "X['num_sents'] = sentences.apply(len)\n",
        "X.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QClWM75qQGPu"
      },
      "source": [
        "X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQTNHAl2P_7-"
      },
      "source": [
        "#### Sentiment analisis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZbN-nQtP-4N"
      },
      "source": [
        "sentiment_analyzer  = SentimentIntensityAnalyzer() \n",
        "sentiment = joined.apply(lambda row: sentiment_analyzer.polarity_scores(row))\n",
        "sentiment = pd.DataFrame.from_records(sentiment)\n",
        "if not any(c == 'neg' for c in X.columns):\n",
        "    X = pd.concat([X, sentiment], axis=1)\n",
        "else:\n",
        "    X.update(sentiment)\n",
        "X.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oszfi3TrrKB8"
      },
      "source": [
        "#### Hatred n-gram dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWBiO3EgrLmQ"
      },
      "source": [
        "def get_weight(row):\n",
        "    return max(hd['prophate'] if hd['ngram'] in row else 0 for i,hd in hatred_dict.iterrows())\n",
        "\n",
        "X['hatedict'] = joined.apply(get_weight) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsVjzWx0QGP0"
      },
      "source": [
        "X['hatedict'].sum(), X['hatedict'].max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THJu41sRrRGY"
      },
      "source": [
        "X.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ASwJzr-WI4H"
      },
      "source": [
        "# 3. Feature selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WU0NZXFK_Pps"
      },
      "source": [
        "'''from functools import reduce\n",
        "from sklearn.impute import SimpleImputer\n",
        "imp = SimpleImputer(strategy='constant', fill_value=0)\n",
        "imp.fit(X_tf)\n",
        "X_tf = imp.transform(X_tf)'''"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmJw7-mxQGP4"
      },
      "source": [
        "X_tf_df = pd.DataFrame.sparse.from_spmatrix(X_tf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCxR7zDQQGP4"
      },
      "source": [
        "X_tf_df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzfP2Uhm7wr2"
      },
      "source": [
        "X = pd.concat([X, X_tf_df], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWWIGNkQ-cAL"
      },
      "source": [
        "X.isna().sum().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmqhCP00QGP7"
      },
      "source": [
        "X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EonJOIwa1XhO"
      },
      "source": [
        "size = len(X.columns)\n",
        "to_cut = ceil(0.7*len(X.columns))\n",
        "to_save = size - to_cut\n",
        "print('100% of features: {}\\n 70% of features: {}'.format(size, to_cut))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMEU6b8P_eFB"
      },
      "source": [
        "from sklearn.feature_selection import SelectKBest, chi2, f_regression\n",
        "\n",
        "selector = SelectKBest(f_regression, k=to_save)\n",
        "selector.fit(X, y)\n",
        "X_new = selector.transform(X)\n",
        "columns = list(X.columns[selector.get_support(indices=True)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLdfYuIhWI-a"
      },
      "source": [
        "columns[:9]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJtsa0X5WKlc"
      },
      "source": [
        "# 4. Classification algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6Tk7nPG-nGI"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(X_new, y, random_state=2, test_size=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjihMjSa80uP"
      },
      "source": [
        "##Random forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPku1HpiHTd1"
      },
      "source": [
        "###Model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WT4MP4yI6QmV"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=0)\n",
        "model.fit(Xtrain, ytrain)\n",
        "ypred_forest = model.predict(Xtest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vr0e8dqKCEHQ"
      },
      "source": [
        "###Test results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ER6ZfAFA8akT"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "mat_forest = confusion_matrix(ytest, ypred_forest)\n",
        "sns.heatmap(mat_forest.T, square=True, annot=True, fmt='d', cbar=False)\n",
        "plt.xlabel('true label')\n",
        "plt.ylabel('predicted label');\n",
        "print(metrics.classification_report(ypred_forest, ytest))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sU-ygTOIBZpO"
      },
      "source": [
        "forest_score = metrics.accuracy_score(ytest, ypred_forest)\n",
        "print(f'{forest_score}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsPEhjG48_L8"
      },
      "source": [
        "##Support vector classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_smgjbmcHBO6"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "model_svc = SVC(kernel='linear', C=1E10)\n",
        "model_svc.fit(Xtrain, ytrain)\n",
        "ypred_svm = model.predict(Xtest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQVIb8nbIrem"
      },
      "source": [
        "###Test results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NbE0bEBItnB"
      },
      "source": [
        "mat_svm = confusion_matrix(ytest, ypred)\n",
        "sns.heatmap(mat_svc.T, square=True, annot=True, fmt='d', cbar=False)\n",
        "plt.xlabel('true label')\n",
        "plt.ylabel('predicted label');\n",
        "print(metrics.classification_report(ytest, ypred_svm))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DziB5Bv8JPkK"
      },
      "source": [
        "svm_score = metrics.accuracy_score(ytest, ypred)\n",
        "print(f'{svm_score}')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}