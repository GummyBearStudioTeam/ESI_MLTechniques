{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia de Copia de NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GummyBearStudioTeam/ESI_MLTechniques/blob/nlp/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NazHONfiVqED"
      },
      "source": [
        "# 1. Preprocessing\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-J2vBvPVbVym"
      },
      "source": [
        "##Word correction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eck8eOglWZa2"
      },
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def get_words(text):\n",
        "  return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "class WordCorrector:\n",
        "\n",
        "\n",
        "  WORDS = Counter(get_words(open('big.txt').read()))\n",
        "\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  ## TODO\n",
        "  def correct_text(self, text):\n",
        "    words = get_words(text)\n",
        "\n",
        "  #WORDS = Counter(words(open('big.txt').read())\n",
        "  def P(self, word, N=sum(WORDS.values())): \n",
        "      \"Probability of `word`.\"\n",
        "      return WORDS[word] / N\n",
        "\n",
        "  def correction(self, word): \n",
        "      \"Most probable spelling correction for word.\"\n",
        "      return max(candidates(word), key=P)\n",
        "\n",
        "  def candidates(self, word): \n",
        "      \"Generate possible spelling corrections for word.\"\n",
        "      return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "\n",
        "  def known(self, words): \n",
        "      \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "      return set(w for w in words if w in WORDS)\n",
        "\n",
        "  def edits1(self, word):\n",
        "      \"All edits that are one edit away from `word`.\"\n",
        "      letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "      splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "      deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "      transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "      replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "      inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "      return set(deletes + transposes + replaces + inserts)\n",
        "  \n",
        "  def edits2(self, word): \n",
        "      \"All edits that are two edits away from `word`.\"\n",
        "      return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9l7NB9VLh1s"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"labeled_data.csv\")\n",
        "hatred_dict = pd.read_csv(\"refined_ngram_dict.csv\")"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GnqAk20H47d"
      },
      "source": [
        "## Cleaning tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhB7nhDMH8vl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16697018-5357-4398-f922-f216a1ddc0c8"
      },
      "source": [
        "df['clean'] = df.apply(lambda row: row['tweet'].lower(), axis=1)\n",
        "\n",
        "df['clean'] = df.apply(lambda row:\n",
        "                        re.sub('[!\"_$%&/()=_ห*ยก@]', '', \n",
        "                        re.sub('(.*rt @\\w+)+:', '',\n",
        "                              row['clean'], flags=re.ASCII)), axis=1)\n",
        "df['clean'][:10]"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     as a woman you shouldn't complain about clean...\n",
              "1     boy dats cold...tyga dwn bad for cuffin dat h...\n",
              "2     you ever fuck a bitch and she start to cry? y...\n",
              "3                     vivabased she look like a tranny\n",
              "4     the shit you hear about me might be true or i...\n",
              "5    tmadisonx: the shit just blows me..claim you s...\n",
              "6    brighterdays: i can not just sit up and hate o...\n",
              "7    #8220;selfiequeenbri: cause i'm tired of you b...\n",
              "8     amp; you might not get ya bitch back amp; tha...\n",
              "9     rhythmixx :hobbies include: fighting mariam\\n...\n",
              "Name: clean, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25O89_lgWAHz"
      },
      "source": [
        "# 2. Vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ASwJzr-WI4H"
      },
      "source": [
        "# 3. Feature selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJtsa0X5WKlc"
      },
      "source": [
        "## 4. Classification algorithm"
      ]
    }
  ]
}