{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N3BBCJdHQGOx"
   },
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kENXF_vPOrD7",
    "outputId": "51a00ff6-669e-4d28-e0ad-aab18ea9b4d1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/gbs/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/gbs/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/gbs/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/gbs/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/gbs/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk.corpus\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import seaborn as sns\n",
    "\n",
    "from math import ceil\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NazHONfiVqED"
   },
   "source": [
    "# 1. Preprocessing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DUSxKRnOU3y"
   },
   "source": [
    "## 1.1. Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "X9l7NB9VLh1s"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"labeled_data.csv\")\n",
    "df.drop(df.columns[0], inplace=True, axis=1)\n",
    "hatred_dict = pd.read_csv(\"refined_ngram_dict.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528
    },
    "id": "44Th0QTQ90sL",
    "outputId": "0f63951e-a79b-4f26-9a50-47f26552c0eb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24778</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>you's a muthaf***in lie &amp;#8220;@LifeAsKing: @2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24779</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>you've gone and broke the wrong heart baby, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24780</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>young buck wanna eat!!.. dat nigguh like I ain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24781</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>youu got wild bitches tellin you lies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24782</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>~~Ruffled | Ntac Eileen Dahlia - Beautiful col...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24783 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       count  hate_speech  offensive_language  neither  class  \\\n",
       "0          3            0                   0        3      2   \n",
       "1          3            0                   3        0      1   \n",
       "2          3            0                   3        0      1   \n",
       "3          3            0                   2        1      1   \n",
       "4          6            0                   6        0      1   \n",
       "...      ...          ...                 ...      ...    ...   \n",
       "24778      3            0                   2        1      1   \n",
       "24779      3            0                   1        2      2   \n",
       "24780      3            0                   3        0      1   \n",
       "24781      6            0                   6        0      1   \n",
       "24782      3            0                   0        3      2   \n",
       "\n",
       "                                                   tweet  \n",
       "0      !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "1      !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
       "2      !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
       "3      !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
       "4      !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  \n",
       "...                                                  ...  \n",
       "24778  you's a muthaf***in lie &#8220;@LifeAsKing: @2...  \n",
       "24779  you've gone and broke the wrong heart baby, an...  \n",
       "24780  young buck wanna eat!!.. dat nigguh like I ain...  \n",
       "24781              youu got wild bitches tellin you lies  \n",
       "24782  ~~Ruffled | Ntac Eileen Dahlia - Beautiful col...  \n",
       "\n",
       "[24783 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "eJRrRp6lQGPD",
    "outputId": "5d4731f4-bd19-4480-bd8b-7fb7939b883b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ngram</th>\n",
       "      <th>prophate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>allah akbar</td>\n",
       "      <td>0.870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blacks</td>\n",
       "      <td>0.583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chink</td>\n",
       "      <td>0.467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chinks</td>\n",
       "      <td>0.542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dykes</td>\n",
       "      <td>0.602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>nigga you a lame</td>\n",
       "      <td>0.556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>niggers are in my</td>\n",
       "      <td>0.714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>wit a lame nigga</td>\n",
       "      <td>0.556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>you a lame bitch</td>\n",
       "      <td>0.556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>you fuck wit a</td>\n",
       "      <td>0.556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ngram  prophate\n",
       "0          allah akbar     0.870\n",
       "1               blacks     0.583\n",
       "2                chink     0.467\n",
       "3               chinks     0.542\n",
       "4                dykes     0.602\n",
       "..                 ...       ...\n",
       "173   nigga you a lame     0.556\n",
       "174  niggers are in my     0.714\n",
       "175   wit a lame nigga     0.556\n",
       "176   you a lame bitch     0.556\n",
       "177     you fuck wit a     0.556\n",
       "\n",
       "[178 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hatred_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fObzoThITjB3",
    "outputId": "18fb7a9d-0398-4d75-a2b2-be305221af3e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count                 0\n",
       "hate_speech           0\n",
       "offensive_language    0\n",
       "neither               0\n",
       "class                 0\n",
       "tweet                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "CdsNfCLD94QD",
    "outputId": "6e10033d-5c0e-42c7-a0fb-5b35ffeb4e80"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>24783.000000</td>\n",
       "      <td>24783.000000</td>\n",
       "      <td>24783.000000</td>\n",
       "      <td>24783.000000</td>\n",
       "      <td>24783.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.243473</td>\n",
       "      <td>0.280515</td>\n",
       "      <td>2.413711</td>\n",
       "      <td>0.549247</td>\n",
       "      <td>1.110277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.883060</td>\n",
       "      <td>0.631851</td>\n",
       "      <td>1.399459</td>\n",
       "      <td>1.113299</td>\n",
       "      <td>0.462089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count   hate_speech  offensive_language       neither  \\\n",
       "count  24783.000000  24783.000000        24783.000000  24783.000000   \n",
       "mean       3.243473      0.280515            2.413711      0.549247   \n",
       "std        0.883060      0.631851            1.399459      1.113299   \n",
       "min        3.000000      0.000000            0.000000      0.000000   \n",
       "25%        3.000000      0.000000            2.000000      0.000000   \n",
       "50%        3.000000      0.000000            3.000000      0.000000   \n",
       "75%        3.000000      0.000000            3.000000      0.000000   \n",
       "max        9.000000      7.000000            9.000000      9.000000   \n",
       "\n",
       "              class  \n",
       "count  24783.000000  \n",
       "mean       1.110277  \n",
       "std        0.462089  \n",
       "min        0.000000  \n",
       "25%        1.000000  \n",
       "50%        1.000000  \n",
       "75%        1.000000  \n",
       "max        2.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_BdK0xmXI9d"
   },
   "source": [
    "## 1.2. Preparing the data to preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "X7BmLnslXIJp"
   },
   "outputs": [],
   "source": [
    "data_set = pd.DataFrame()\n",
    "data_set['clean'] = df['tweet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVIKSahQWo4U"
   },
   "source": [
    "### Limiting processed data\n",
    "To reduce memory usage a data limit can be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "dG2J5IG_WlaA"
   },
   "outputs": [],
   "source": [
    "N = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "vbRzbg6IWn-P"
   },
   "outputs": [],
   "source": [
    "if N is not None:\n",
    "    data_set = data_set[0:N]\n",
    "    df = df[0:N]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1AJs3MehsYg"
   },
   "source": [
    "## 1.3. Correction dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WLOQHuQh7mX"
   },
   "source": [
    "### Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "bp1lbiujhm6V",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "bp1lbiujhm6V"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"ain't\": 'aint',\n",
       " \"aren't\": 'arent',\n",
       " \"can't\": 'cant',\n",
       " \"can't've\": 'cantve',\n",
       " \"'cause\": 'cause',\n",
       " \"could've\": 'couldve',\n",
       " \"couldn't\": 'couldnt',\n",
       " \"couldn't've\": 'couldntve',\n",
       " \"didn't\": 'didnt',\n",
       " \"doesn't\": 'doesnt',\n",
       " \"don't\": 'dont',\n",
       " \"hadn't\": 'hadnt',\n",
       " \"hadn't've\": 'hadntve',\n",
       " \"hasn't\": 'hasnt',\n",
       " \"haven't\": 'havent',\n",
       " \"he'd\": 'hed',\n",
       " \"he'd've\": 'hedve',\n",
       " \"he'll\": 'hell',\n",
       " \"he'll've\": 'hellve',\n",
       " \"he's\": 'hes',\n",
       " \"how'd\": 'howd',\n",
       " \"how'd'y\": 'howdy',\n",
       " \"how'll\": 'howll',\n",
       " \"how's\": 'hows',\n",
       " \"I'd\": 'Id',\n",
       " \"I'd've\": 'Idve',\n",
       " \"I'll\": 'Ill',\n",
       " \"I'll've\": 'Illve',\n",
       " \"I'm\": 'Im',\n",
       " \"I've\": 'Ive',\n",
       " \"isn't\": 'isnt',\n",
       " \"it'd\": 'itd',\n",
       " \"it'd've\": 'itdve',\n",
       " \"it'll\": 'itll',\n",
       " \"it'll've\": 'itllve',\n",
       " \"it's\": 'its',\n",
       " \"let's\": 'lets',\n",
       " \"ma'am\": 'maam',\n",
       " \"mayn't\": 'maynt',\n",
       " \"might've\": 'mightve',\n",
       " \"mightn't\": 'mightnt',\n",
       " \"mightn't've\": 'mightntve',\n",
       " \"must've\": 'mustve',\n",
       " \"mustn't\": 'mustnt',\n",
       " \"mustn't've\": 'mustntve',\n",
       " \"needn't\": 'neednt',\n",
       " \"needn't've\": 'needntve',\n",
       " \"o'clock\": 'oclock',\n",
       " \"oughtn't\": 'oughtnt',\n",
       " \"oughtn't've\": 'oughtntve',\n",
       " \"shan't\": 'shant',\n",
       " \"sha'n't\": 'shant',\n",
       " \"shan't've\": 'shantve',\n",
       " \"she'd\": 'shed',\n",
       " \"she'd've\": 'shedve',\n",
       " \"she'll\": 'shell',\n",
       " \"she'll've\": 'shellve',\n",
       " \"she's\": 'shes',\n",
       " \"should've\": 'shouldve',\n",
       " \"shouldn't\": 'shouldnt',\n",
       " \"shouldn't've\": 'shouldntve',\n",
       " \"so've\": 'sove',\n",
       " \"so's\": 'sos',\n",
       " \"that'd\": 'thatd',\n",
       " \"that'd've\": 'thatdve',\n",
       " \"that's\": 'thats',\n",
       " \"there'd\": 'thered',\n",
       " \"there'd've\": 'theredve',\n",
       " \"there's\": 'theres',\n",
       " \"they'd\": 'theyd',\n",
       " \"they'd've\": 'theydve',\n",
       " \"they'll\": 'theyll',\n",
       " \"they'll've\": 'theyllve',\n",
       " \"they're\": 'theyre',\n",
       " \"they've\": 'theyve',\n",
       " \"to've\": 'tove',\n",
       " \"wasn't\": 'wasnt',\n",
       " \"we'd\": 'wed',\n",
       " \"we'd've\": 'wedve',\n",
       " \"we'll\": 'well',\n",
       " \"we'll've\": 'wellve',\n",
       " \"we're\": 'were',\n",
       " \"we've\": 'weve',\n",
       " \"weren't\": 'werent',\n",
       " \"what'll\": 'whatll',\n",
       " \"what'll've\": 'whatllve',\n",
       " \"what're\": 'whatre',\n",
       " \"what's\": 'whats',\n",
       " \"what've\": 'whatve',\n",
       " \"when's\": 'whens',\n",
       " \"when've\": 'whenve',\n",
       " \"where'd\": 'whered',\n",
       " \"where's\": 'wheres',\n",
       " \"where've\": 'whereve',\n",
       " \"who'll\": 'wholl',\n",
       " \"who'll've\": 'whollve',\n",
       " \"who's\": 'whos',\n",
       " \"who've\": 'whove',\n",
       " \"why's\": 'whys',\n",
       " \"why've\": 'whyve',\n",
       " \"will've\": 'willve',\n",
       " \"won't\": 'wont',\n",
       " \"won't've\": 'wontve',\n",
       " \"would've\": 'wouldve',\n",
       " \"wouldn't\": 'wouldnt',\n",
       " \"wouldn't've\": 'wouldntve',\n",
       " \"y'all\": 'yall',\n",
       " \"y'all'd\": 'yalld',\n",
       " \"y'all'd've\": 'yalldve',\n",
       " \"y'all're\": 'yallre',\n",
       " \"y'all've\": 'yallve',\n",
       " \"you'd\": 'youd',\n",
       " \"you'd've\": 'youdve',\n",
       " \"you'll\": 'youll',\n",
       " \"you'll've\": 'youllve',\n",
       " \"you're\": 'youre',\n",
       " \"you've\": 'youve'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{d: d.replace(\"'\", '') for d in contractions.keys()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ZdP2WaTjTIm"
   },
   "source": [
    "### Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "fV2XR4mBjSNc",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "adjustments = {\n",
    "    'ya': 'you',\n",
    "    'aw': '',\n",
    "    'dat': 'that',\n",
    "    'dem': 'them',\n",
    "    'll': 'will',\n",
    "    'u': 'you'\n",
    "}\n",
    "\n",
    "abbreviations = {\n",
    "    'idk': 'I do not know',\n",
    "    'btw': 'by the way',\n",
    "    'pls': 'please'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZ3loh-2YyXK"
   },
   "source": [
    "### Emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "TEBQS--eYwxS",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "emoticons = {\n",
    "    '&#8220': '', #\"\n",
    "    '&#128555': 'tired',\n",
    "    '&#128553': 'weary',\n",
    "    '&#128557': 'crying',\n",
    "    '&#128514': 'joy',\n",
    "    '&#128131': '',\n",
    "    '&#128149': 'love',\n",
    "    '&#128095': '',\n",
    "    '&#128128': 'death',\n",
    "    '&#127813': '',\n",
    "    '&#127829': '',\n",
    "    '&#128064': '',\n",
    "    '&#128073': '',\n",
    "    '&#128077': 'ok',\n",
    "    '&#127867': '',\n",
    "    '&#9733': '',\n",
    "    '&#127942': '',\n",
    "    '&#128034': '',\n",
    "    '&#128072': '',\n",
    "    '&#128075': '',\n",
    "    '&#128530': 'unamused',\n",
    "    '&#128563': '', #hands up\n",
    "    '&#128175': '',\n",
    "    '&#128588': '', #hands up\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ysMH1zIvoz9y",
    "outputId": "57306891-5ff5-444a-c953-98be74793946"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 &#128079; congrats you've turned a hoe into a housewife, don't get shitty when your guys start singing they hit it first. #ButThatsNoneOfMyBusiness\n",
      "1 &#128165;&#128162; on the pussy http://t.co/mWXQnjm4So\n",
      "2 &#128347; is the most important thing. All this temporary bullshit and lies is fa the birds. Kill that !\n",
      "3 &#128520;&#127383; we snap chatted for one night lol. But you're cute. Snapchat me back nig\n",
      "4 &#128527; haahaa ,dumb bitch\n",
      "5 &#128532; RT @MichyDoe: Every week them hoes partying ! I see them hoes in every city partying for an event\n",
      "6 &#128539;&#128120; you've been a good as friend to me , glad I got your honkie ass. Let's fuck shit up this year\n",
      "7 &#128540;&#128583;&#128582; I hate ya bitch ass\n",
      "8 &#128583;&#128583;&#128583;&#128583; can y'all females let this sink in for min? Moment of silence to get y'all bitches thinking right? http://t.co/cDN0dEYCkO\n",
      "9 &#128700;&#128700;- you a little genius man i need you for a class so u can help a nig out but you cool af man\n",
      "10 &#8216;Chillin&#8217; With My Homie Or What&#8217;s Left Of Him&#8217;: British Rapper Turned ISIS Jihadi Poses With Severed Head http://t.co/L9vMdNOXPg #tcot\n",
      "11 &#9889;&#65039;&#9889;&#65039; why lie you're my nigguh &#128526;\n",
      "12 &#9996;&#65039; out bitch I'm winning &#128526; http://t.co/k3l15zoBGz\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for tweet in df['tweet']:\n",
    "    if re.match('&#\\d{4}', tweet):\n",
    "        if any(True for emot in emoticons if emot in tweet):\n",
    "            continue\n",
    "        print(i, tweet)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fCrAY6mOOdOM"
   },
   "source": [
    "## 1.4. Correcting words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "28ygMggGOPdp"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_words(text):\n",
    "    return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "def replace_jargon(word):\n",
    "    for jargon_dict in [contractions, abbreviations, adjustments]:\n",
    "        if word in jargon_dict:\n",
    "            return jargon_dict[word]\n",
    "    return word\n",
    "\n",
    "def replace_jargon_from_array(arr):\n",
    "    return [replace_jargon(w) for w in arr]\n",
    "\n",
    "def replace_jargon_from_text(text):\n",
    "    words = get_words(text)\n",
    "    return replace_jargon_from_array(words)\n",
    "\n",
    "def delete_multiplied_letters(arr):\n",
    "    changed = []\n",
    "    for word in arr:\n",
    "        word = re.sub(r'(\\w)\\1{2,}', r'\\1', word)\n",
    "        changed.append(word)\n",
    "    return changed\n",
    "\n",
    "WORDS = Counter(get_words(open('big.txt').read()))\n",
    "\n",
    "def correct_array(words):\n",
    "    return [(correction(w) if w not in WORDS else w) for w in words]\n",
    "\n",
    "def correct_text(text):\n",
    "    words = get_words(text)\n",
    "    return correct_array(words)\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    best = max(candidates(word), key=P)\n",
    "    #if best == 'a':\n",
    "        #print(word, '=>', best)\n",
    "    return best\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    \n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    #inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces)\n",
    "  \n",
    "def edits_gen(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits(word) for e2 in edits(e1))\n",
    "\n",
    "## Lemmatizing\n",
    "\n",
    "def lemmatize_array(arr):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in arr]\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return lemmatize_array(get_words(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_GnqAk20H47d"
   },
   "source": [
    "## 1.5. Cleaning tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "fhB7nhDMH8vl"
   },
   "outputs": [],
   "source": [
    "special_characters_regex = '[!\"_$%&/()=_Ë†*Â¡@' ',:;?#]'\n",
    "retweet_regex = '(.*rt @\\w+)+:'\n",
    "space_regex = '\\s+'\n",
    "url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "             '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "emoticon_regex = '&#\\d+;'\n",
    "mention_regex = '@[\\w\\-]+'\n",
    "number_regex = '\\d+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "rYQVczlUQGPN"
   },
   "outputs": [],
   "source": [
    "data_set['clean'] = data_set.apply(lambda row:\n",
    "                        re.sub(space_regex, ' ',\n",
    "                        re.sub(special_characters_regex, '', \n",
    "                        re.sub(number_regex, ' NUMBERHERE ',\n",
    "                        re.sub('\\s*RT MENTIONHERE', ' MENTIONHERE ',\n",
    "                        re.sub(url_regex, ' LINKHERE ',\n",
    "                        re.sub(mention_regex, ' MENTIONHERE ',\n",
    "                        re.sub(retweet_regex, '',\n",
    "                        re.sub(space_regex, ' ',\n",
    "                              row['clean'])))))), flags=re.ASCII)), axis=1)\n",
    "\n",
    "data_set['clean'] = data_set.apply(lambda row: row['clean'].lower(), axis=1)\n",
    "data_set['clean'] = data_set.apply(lambda row: replace_jargon_from_text(row['clean']), axis=1)\n",
    "data_set['clean'] = data_set.apply(lambda row: delete_multiplied_letters(row['clean']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y5xB2vwFRkyX"
   },
   "source": [
    "## 1.6. Word Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TNoqm2rEaER5",
    "outputId": "059c67b3-4f89-49ce-91fc-24540b947e0a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [rt, mentionhere, as, a, woman, you, shouldn, ...\n",
       "1        [rt, mentionhere, boy, dats, cold, tyga, dwn, ...\n",
       "2        [rt, mentionhere, dawg, rt, mentionhere, you, ...\n",
       "3        [rt, mentionhere, mentionhere, she, look, like...\n",
       "4        [rt, mentionhere, the, shit, you, hear, about,...\n",
       "                               ...                        \n",
       "24778    [you, s, a, muthafin, lie, numberhere, mention...\n",
       "24779    [you, ve, gone, and, broke, the, wrong, heart,...\n",
       "24780    [young, buck, wanna, eat, that, nigguh, like, ...\n",
       "24781        [youu, got, wild, bitches, tellin, you, lies]\n",
       "24782    [ruffled, ntac, eileen, dahlia, beautiful, col...\n",
       "Name: clean, Length: 24783, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set['clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "LMABvmnBRkAX"
   },
   "outputs": [],
   "source": [
    "data_set['clean'] = data_set.apply(lambda row: correct_array(row['clean']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "6HpQbw0DOcj9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [it, mentionhere, as, a, woman, you, shouldn, ...\n",
       "1     [it, mentionhere, boy, days, cold, tea, own, b...\n",
       "2     [it, mentionhere, dawn, it, mentionhere, you, ...\n",
       "3     [it, mentionhere, mentionhere, she, look, like...\n",
       "4     [it, mentionhere, the, shit, you, hear, about,...\n",
       "5     [mentionhere, the, shit, just, blows, me, clai...\n",
       "6     [mentionhere, i, can, not, just, sit, up, and,...\n",
       "7     [numberhere, mentionhere, cause, i, m, tired, ...\n",
       "8     [amp, you, might, not, get, you, bitch, back, ...\n",
       "9     [mentionhere, hobbies, include, fighting, mari...\n",
       "10    [weeks, is, a, bitch, she, curves, everyone, l...\n",
       "11                [marya, gang, bitch, its, gang, land]\n",
       "12    [so, hoes, that, smoke, are, loses, yea, go, o...\n",
       "13    [bad, birches, is, the, only, thing, that, i, ...\n",
       "14                            [bitch, get, up, off, me]\n",
       "15                   [bitch, nigga, miss, me, with, it]\n",
       "16                               [bitch, ply, whatever]\n",
       "17                          [bitch, who, do, you, love]\n",
       "18                [birches, get, cut, off, everyday, b]\n",
       "19                  [black, bottle, amp, a, bad, bitch]\n",
       "20               [broke, bitch, can, tell, me, nothing]\n",
       "21                    [cancel, that, bitch, like, nine]\n",
       "22           [can, you, see, these, hoes, wont, change]\n",
       "23    [fuck, no, that, bitch, dont, even, suck, dick...\n",
       "24    [got, you, bitch, tip, toying, on, my, hardwoo...\n",
       "25    [her, pussy, lips, like, heaven, doors, number...\n",
       "26                        [he, what, its, hitting, for]\n",
       "27    [i, met, that, pussy, on, ocean, dr, i, gave, ...\n",
       "28    [i, need, a, tipsy, bitch, who, fuck, on, henn...\n",
       "29    [i, spend, my, money, how, i, want, bitch, its...\n",
       "Name: clean, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set['clean'][:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPfeavePQGPS"
   },
   "source": [
    "### 1.6.1. Setup working sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "lmK81KSN8h0b"
   },
   "outputs": [],
   "source": [
    "joined = data_set.apply(lambda row: ' '.join(row['clean']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "ZukT4a0nQGPh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        it mentionhere as a woman you shouldn t compla...\n",
       "1        it mentionhere boy days cold tea own bad for c...\n",
       "2        it mentionhere dawn it mentionhere you ever fu...\n",
       "3        it mentionhere mentionhere she look like a granny\n",
       "4        it mentionhere the shit you hear about me migh...\n",
       "                               ...                        \n",
       "24778    you s a muthafin lie numberhere mentionhere me...\n",
       "24779    you ve gone and broke the wrong heart baby and...\n",
       "24780    young buck anna eat that nigh like i aunt muci...\n",
       "24781                   you got wild birches tell you lies\n",
       "24782    ruffled near spleen dahlia beautiful color com...\n",
       "Length: 24783, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "zeof13oPxGO5"
   },
   "outputs": [],
   "source": [
    "y = df[['class']]\n",
    "X = pd.DataFrame() #df[['hate_speech', 'offensive_language', 'neither']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oszfi3TrrKB8"
   },
   "source": [
    "### 1.6.2. Hatred n-gram dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "vWBiO3EgrLmQ"
   },
   "outputs": [],
   "source": [
    "def get_weight(row):\n",
    "    return max(hd['prophate'] if hd['ngram'] in row else 0 for i,hd in hatred_dict.iterrows())\n",
    "\n",
    "X['hatedict'] = joined.apply(get_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "QsVjzWx0QGP0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "764.4540000000001"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['hatedict'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "THJu41sRrRGY"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hatedict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>24783.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.030846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.132679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.889000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           hatedict\n",
       "count  24783.000000\n",
       "mean       0.030846\n",
       "std        0.132679\n",
       "min        0.000000\n",
       "25%        0.000000\n",
       "50%        0.000000\n",
       "75%        0.000000\n",
       "max        0.889000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0m-Qatsm7cHo"
   },
   "source": [
    "## 1.7. Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "r5lyy-vN7ZDX"
   },
   "outputs": [],
   "source": [
    "data_set['clean'] = data_set.apply(lambda row: lemmatize_array(row['clean']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "BpmD254bZeSi"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [it, mentionhere, a, a, woman, you, shouldn, t...\n",
       "1        [it, mentionhere, boy, day, cold, tea, own, ba...\n",
       "2        [it, mentionhere, dawn, it, mentionhere, you, ...\n",
       "3        [it, mentionhere, mentionhere, she, look, like...\n",
       "4        [it, mentionhere, the, shit, you, hear, about,...\n",
       "                               ...                        \n",
       "24778    [you, s, a, muthafin, lie, numberhere, mention...\n",
       "24779    [you, ve, gone, and, broke, the, wrong, heart,...\n",
       "24780    [young, buck, anna, eat, that, nigh, like, i, ...\n",
       "24781              [you, got, wild, birch, tell, you, lie]\n",
       "24782    [ruffled, near, spleen, dahlia, beautiful, col...\n",
       "Name: clean, Length: 24783, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set['clean']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1v3o8k_LcJi"
   },
   "source": [
    "## 1.8. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "-AOni-T8LEBq"
   },
   "outputs": [],
   "source": [
    "sentences = data_set.apply(lambda row: sent_tokenize(' '.join(row['clean'])),axis=1)\n",
    "words = data_set.apply(lambda row: word_tokenize(' '.join(row['clean'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "BKKo7WxjOGI-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [mentionhere, woman, complain, cleaning, house...\n",
       "1        [mentionhere, boy, day, cold, tea, bad, coffin...\n",
       "2        [mentionhere, dawn, mentionhere, ever, fuck, b...\n",
       "3           [mentionhere, mentionhere, look, like, granny]\n",
       "4        [mentionhere, shit, hear, might, true, might, ...\n",
       "                               ...                        \n",
       "24778    [muthafin, lie, numberhere, mentionhere, menti...\n",
       "24779    [gone, broke, wrong, heart, baby, drove, redne...\n",
       "24780    [young, buck, anna, eat, nigh, like, aunt, muc...\n",
       "24781                        [got, wild, birch, tell, lie]\n",
       "24782    [ruffled, near, spleen, dahlia, beautiful, col...\n",
       "Name: clean, Length: 24783, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords += [',', '.', ';']\n",
    "data_set['clean'] = words.apply(lambda row: [w for w in row if w not in stopwords]) \n",
    "data_set['clean']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lySCoviQHBvv"
   },
   "source": [
    "# 2. Vectorization\n",
    "\n",
    "## 2.1. TFiDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PMVh1GlfFXbI"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=1)\n",
    "X_tf = vectorizer.fit_transform(joined)\n",
    "X_tf.column = vectorizer.get_feature_names()\n",
    "X_tf.toarray()[0]\n",
    "#X.column\n",
    "X_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g47n-vHbQGPi"
   },
   "outputs": [],
   "source": [
    "X_tf.column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Sbad0IWHGGP"
   },
   "source": [
    "## 2.2. TFiDF + N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AZwZLemXDJJk"
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(2,3), min_df=1)\n",
    "X_ngram = vectorizer.fit_transform(joined)\n",
    "X_ngram.column = vectorizer.get_feature_names()\n",
    "X_ngram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e_da_pPHQGPn"
   },
   "outputs": [],
   "source": [
    "X_ngram.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pnlnj_DSHIsT"
   },
   "source": [
    "## 2.3. TFiDF + N-grams + POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8MdhGYTFfAtg"
   },
   "outputs": [],
   "source": [
    "tagged = data_set['clean'].apply(nltk.pos_tag)\n",
    "tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8C9Ap1rsHNa6"
   },
   "source": [
    "## 2.4. Other Features\n",
    "\n",
    "#### RTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ayPzlxtRJWqS"
   },
   "outputs": [],
   "source": [
    "X['RT'] = df.apply(lambda row: row[\"tweet\"].count(\"RT\") , axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CsEHbtXIMzVI"
   },
   "source": [
    "#### Number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R7JA26kaMykj"
   },
   "outputs": [],
   "source": [
    "X['num_words'] = words.apply(len)\n",
    "X.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v7bU2ZJSNXHg"
   },
   "source": [
    "#### Number of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7bay_BUQNZdP"
   },
   "outputs": [],
   "source": [
    "X['num_sents'] = sentences.apply(len)\n",
    "X.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QClWM75qQGPu"
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQTNHAl2P_7-"
   },
   "source": [
    "#### Sentiment analisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ZbN-nQtP-4N"
   },
   "outputs": [],
   "source": [
    "sentiment_analyzer  = SentimentIntensityAnalyzer() \n",
    "sentiment = joined.apply(lambda row: sentiment_analyzer.polarity_scores(row))\n",
    "sentiment = pd.DataFrame.from_records(sentiment)\n",
    "if not any(c == 'neg' for c in X.columns):\n",
    "    X = pd.concat([X, sentiment], axis=1)\n",
    "else:\n",
    "    X.update(sentiment)\n",
    "X.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ASwJzr-WI4H"
   },
   "source": [
    "# 3. Feature selection"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "WU0NZXFK_Pps"
   },
   "source": [
    "from functools import reduce\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(strategy='constant', fill_value=0)\n",
    "imp.fit(X_tf)\n",
    "X_tf = imp.transform(X_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kmJw7-mxQGP4"
   },
   "outputs": [],
   "source": [
    "X_tf_df = pd.DataFrame.sparse.from_spmatrix(X_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VCxR7zDQQGP4"
   },
   "outputs": [],
   "source": [
    "X_tf_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mzfP2Uhm7wr2"
   },
   "outputs": [],
   "source": [
    "X = pd.concat([X, X_tf_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nWWIGNkQ-cAL"
   },
   "outputs": [],
   "source": [
    "X.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BmqhCP00QGP7"
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EonJOIwa1XhO"
   },
   "outputs": [],
   "source": [
    "size = len(X.columns)\n",
    "to_cut = ceil(0.7*len(X.columns))\n",
    "to_save = size - to_cut\n",
    "print('100% of features: {}\\n 70% of features: {}'.format(size, to_cut))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uMEU6b8P_eFB"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2, f_regression\n",
    "\n",
    "selector = SelectKBest(f_regression, k=to_save)\n",
    "selector.fit(X, y)\n",
    "X_new = selector.transform(X)\n",
    "columns = list(X.columns[selector.get_support(indices=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RLdfYuIhWI-a"
   },
   "outputs": [],
   "source": [
    "columns[:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJtsa0X5WKlc"
   },
   "source": [
    "# 4. Classification algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_6Tk7nPG-nGI"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X_new, y, random_state=2, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lPku1HpiHTd1"
   },
   "source": [
    "## 4.1. Random forest\n",
    "\n",
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WT4MP4yI6QmV"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "model.fit(Xtrain, ytrain)\n",
    "ypred_forest = model.predict(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vr0e8dqKCEHQ"
   },
   "source": [
    "### Test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ER6ZfAFA8akT"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mat_forest = confusion_matrix(ytest, ypred_forest)\n",
    "sns.heatmap(mat_forest.T, square=True, annot=True, fmt='d', cbar=False)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');\n",
    "print(metrics.classification_report(ypred_forest, ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sU-ygTOIBZpO"
   },
   "outputs": [],
   "source": [
    "forest_score = metrics.accuracy_score(ytest, ypred_forest)\n",
    "print(f'{forest_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZsPEhjG48_L8"
   },
   "source": [
    "## 4.2. Support vector classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_smgjbmcHBO6"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "model_svc = SVC(kernel='linear', C=1E10)\n",
    "model_svc.fit(Xtrain, ytrain)\n",
    "ypred_svm = model.predict(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQVIb8nbIrem"
   },
   "source": [
    "### Test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2NbE0bEBItnB"
   },
   "outputs": [],
   "source": [
    "mat_svm = confusion_matrix(ytest, ypred)\n",
    "sns.heatmap(mat_svc.T, square=True, annot=True, fmt='d', cbar=False)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');\n",
    "print(metrics.classification_report(ytest, ypred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DziB5Bv8JPkK"
   },
   "outputs": [],
   "source": [
    "svm_score = metrics.accuracy_score(ytest, ypred)\n",
    "print(f'{svm_score}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "9WLOQHuQh7mX",
    "3ZdP2WaTjTIm",
    "aZ3loh-2YyXK"
   ],
   "name": "NLP(5).ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
