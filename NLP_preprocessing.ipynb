{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N3BBCJdHQGOx"
   },
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kENXF_vPOrD7",
    "outputId": "51a00ff6-669e-4d28-e0ad-aab18ea9b4d1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/gbs/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/gbs/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/gbs/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/gbs/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/gbs/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk.corpus\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from math import ceil\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NazHONfiVqED"
   },
   "source": [
    "# 1. Preprocessing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DUSxKRnOU3y"
   },
   "source": [
    "## 1.1. Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "X9l7NB9VLh1s"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"labeled_data.csv\")\n",
    "df.drop(df.columns[0], inplace=True, axis=1)\n",
    "hatred_dict = pd.read_csv(\"refined_ngram_dict.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528
    },
    "id": "44Th0QTQ90sL",
    "outputId": "0f63951e-a79b-4f26-9a50-47f26552c0eb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24778</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>you's a muthaf***in lie &amp;#8220;@LifeAsKing: @2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24779</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>you've gone and broke the wrong heart baby, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24780</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>young buck wanna eat!!.. dat nigguh like I ain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24781</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>youu got wild bitches tellin you lies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24782</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>~~Ruffled | Ntac Eileen Dahlia - Beautiful col...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24783 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       count  hate_speech  offensive_language  neither  class  \\\n",
       "0          3            0                   0        3      2   \n",
       "1          3            0                   3        0      1   \n",
       "2          3            0                   3        0      1   \n",
       "3          3            0                   2        1      1   \n",
       "4          6            0                   6        0      1   \n",
       "...      ...          ...                 ...      ...    ...   \n",
       "24778      3            0                   2        1      1   \n",
       "24779      3            0                   1        2      2   \n",
       "24780      3            0                   3        0      1   \n",
       "24781      6            0                   6        0      1   \n",
       "24782      3            0                   0        3      2   \n",
       "\n",
       "                                                   tweet  \n",
       "0      !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "1      !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
       "2      !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
       "3      !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
       "4      !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  \n",
       "...                                                  ...  \n",
       "24778  you's a muthaf***in lie &#8220;@LifeAsKing: @2...  \n",
       "24779  you've gone and broke the wrong heart baby, an...  \n",
       "24780  young buck wanna eat!!.. dat nigguh like I ain...  \n",
       "24781              youu got wild bitches tellin you lies  \n",
       "24782  ~~Ruffled | Ntac Eileen Dahlia - Beautiful col...  \n",
       "\n",
       "[24783 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "eJRrRp6lQGPD",
    "outputId": "5d4731f4-bd19-4480-bd8b-7fb7939b883b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ngram</th>\n",
       "      <th>prophate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>allah akbar</td>\n",
       "      <td>0.870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blacks</td>\n",
       "      <td>0.583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chink</td>\n",
       "      <td>0.467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chinks</td>\n",
       "      <td>0.542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dykes</td>\n",
       "      <td>0.602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>nigga you a lame</td>\n",
       "      <td>0.556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>niggers are in my</td>\n",
       "      <td>0.714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>wit a lame nigga</td>\n",
       "      <td>0.556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>you a lame bitch</td>\n",
       "      <td>0.556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>you fuck wit a</td>\n",
       "      <td>0.556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ngram  prophate\n",
       "0          allah akbar     0.870\n",
       "1               blacks     0.583\n",
       "2                chink     0.467\n",
       "3               chinks     0.542\n",
       "4                dykes     0.602\n",
       "..                 ...       ...\n",
       "173   nigga you a lame     0.556\n",
       "174  niggers are in my     0.714\n",
       "175   wit a lame nigga     0.556\n",
       "176   you a lame bitch     0.556\n",
       "177     you fuck wit a     0.556\n",
       "\n",
       "[178 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hatred_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fObzoThITjB3",
    "outputId": "18fb7a9d-0398-4d75-a2b2-be305221af3e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count                 0\n",
       "hate_speech           0\n",
       "offensive_language    0\n",
       "neither               0\n",
       "class                 0\n",
       "tweet                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "CdsNfCLD94QD",
    "outputId": "6e10033d-5c0e-42c7-a0fb-5b35ffeb4e80"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>24783.000000</td>\n",
       "      <td>24783.000000</td>\n",
       "      <td>24783.000000</td>\n",
       "      <td>24783.000000</td>\n",
       "      <td>24783.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.243473</td>\n",
       "      <td>0.280515</td>\n",
       "      <td>2.413711</td>\n",
       "      <td>0.549247</td>\n",
       "      <td>1.110277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.883060</td>\n",
       "      <td>0.631851</td>\n",
       "      <td>1.399459</td>\n",
       "      <td>1.113299</td>\n",
       "      <td>0.462089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count   hate_speech  offensive_language       neither  \\\n",
       "count  24783.000000  24783.000000        24783.000000  24783.000000   \n",
       "mean       3.243473      0.280515            2.413711      0.549247   \n",
       "std        0.883060      0.631851            1.399459      1.113299   \n",
       "min        3.000000      0.000000            0.000000      0.000000   \n",
       "25%        3.000000      0.000000            2.000000      0.000000   \n",
       "50%        3.000000      0.000000            3.000000      0.000000   \n",
       "75%        3.000000      0.000000            3.000000      0.000000   \n",
       "max        9.000000      7.000000            9.000000      9.000000   \n",
       "\n",
       "              class  \n",
       "count  24783.000000  \n",
       "mean       1.110277  \n",
       "std        0.462089  \n",
       "min        0.000000  \n",
       "25%        1.000000  \n",
       "50%        1.000000  \n",
       "75%        1.000000  \n",
       "max        2.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_BdK0xmXI9d"
   },
   "source": [
    "## 1.2. Preparing the data to preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "X7BmLnslXIJp"
   },
   "outputs": [],
   "source": [
    "data_set = pd.DataFrame()\n",
    "data_set['clean'] = df['tweet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVIKSahQWo4U"
   },
   "source": [
    "### Limiting processed data\n",
    "To reduce memory usage a data limit can be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "dG2J5IG_WlaA"
   },
   "outputs": [],
   "source": [
    "N = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "vbRzbg6IWn-P"
   },
   "outputs": [],
   "source": [
    "if N is not None:\n",
    "    data_set = data_set[0:N]\n",
    "    df = df[0:N]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1AJs3MehsYg"
   },
   "source": [
    "## 1.3. Correction dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WLOQHuQh7mX"
   },
   "source": [
    "### Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "bp1lbiujhm6V",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "bp1lbiujhm6V"
   },
   "outputs": [],
   "source": [
    "ext = {d:d.replace(\"'\", '') for d in contractions.keys()}\n",
    "contractions.update({ext[d]:contractions[d] for d in ext.keys()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ZdP2WaTjTIm"
   },
   "source": [
    "### Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "fV2XR4mBjSNc",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "adjustments = {\n",
    "    'ya': 'you',\n",
    "    'aw': '',\n",
    "    'dat': 'that',\n",
    "    'dem': 'them',\n",
    "    'll': 'will',\n",
    "    'u': 'you'\n",
    "}\n",
    "\n",
    "abbreviations = {\n",
    "    'idk': 'I do not know',\n",
    "    'btw': 'by the way',\n",
    "    'pls': 'please'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZ3loh-2YyXK"
   },
   "source": [
    "### Emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "TEBQS--eYwxS",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "emoticons = {\n",
    "    '&#8220': '', #\"\n",
    "    '&#128555': 'tired',\n",
    "    '&#128553': 'weary',\n",
    "    '&#128557': 'crying',\n",
    "    '&#128514': 'joy',\n",
    "    '&#128131': '',\n",
    "    '&#128149': 'love',\n",
    "    '&#128095': '',\n",
    "    '&#128128': 'death',\n",
    "    '&#127813': '',\n",
    "    '&#127829': '',\n",
    "    '&#128064': '',\n",
    "    '&#128073': '',\n",
    "    '&#128077': 'ok',\n",
    "    '&#127867': '',\n",
    "    '&#9733': '',\n",
    "    '&#127942': '',\n",
    "    '&#128034': '',\n",
    "    '&#128072': '',\n",
    "    '&#128075': '',\n",
    "    '&#128530': 'unamused',\n",
    "    '&#128563': '', #hands up\n",
    "    '&#128175': '',\n",
    "    '&#128588': '', #hands up\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ysMH1zIvoz9y",
    "outputId": "57306891-5ff5-444a-c953-98be74793946"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 &#128079; congrats you've turned a hoe into a housewife, don't get shitty when your guys start singing they hit it first. #ButThatsNoneOfMyBusiness\n",
      "1 &#128165;&#128162; on the pussy http://t.co/mWXQnjm4So\n",
      "2 &#128347; is the most important thing. All this temporary bullshit and lies is fa the birds. Kill that !\n",
      "3 &#128520;&#127383; we snap chatted for one night lol. But you're cute. Snapchat me back nig\n",
      "4 &#128527; haahaa ,dumb bitch\n",
      "5 &#128532; RT @MichyDoe: Every week them hoes partying ! I see them hoes in every city partying for an event\n",
      "6 &#128539;&#128120; you've been a good as friend to me , glad I got your honkie ass. Let's fuck shit up this year\n",
      "7 &#128540;&#128583;&#128582; I hate ya bitch ass\n",
      "8 &#128583;&#128583;&#128583;&#128583; can y'all females let this sink in for min? Moment of silence to get y'all bitches thinking right? http://t.co/cDN0dEYCkO\n",
      "9 &#128700;&#128700;- you a little genius man i need you for a class so u can help a nig out but you cool af man\n",
      "10 &#8216;Chillin&#8217; With My Homie Or What&#8217;s Left Of Him&#8217;: British Rapper Turned ISIS Jihadi Poses With Severed Head http://t.co/L9vMdNOXPg #tcot\n",
      "11 &#9889;&#65039;&#9889;&#65039; why lie you're my nigguh &#128526;\n",
      "12 &#9996;&#65039; out bitch I'm winning &#128526; http://t.co/k3l15zoBGz\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for tweet in df['tweet']:\n",
    "    if re.match('&#\\d{4}', tweet):\n",
    "        if any(True for emot in emoticons if emot in tweet):\n",
    "            continue\n",
    "        print(i, tweet)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fCrAY6mOOdOM"
   },
   "source": [
    "## 1.4. Correcting words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "28ygMggGOPdp"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_words(text):\n",
    "    return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "def replace_jargon(word):\n",
    "    for jargon_dict in [contractions, abbreviations, adjustments]:\n",
    "        if word in jargon_dict:\n",
    "            return jargon_dict[word]\n",
    "    return word\n",
    "\n",
    "def replace_jargon_from_array(arr):\n",
    "    return [replace_jargon(w) for w in arr]\n",
    "\n",
    "def replace_jargon_from_text(text):\n",
    "    words = get_words(text)\n",
    "    return replace_jargon_from_array(words)\n",
    "\n",
    "def delete_multiplied_letters(arr):\n",
    "    changed = []\n",
    "    for word in arr:\n",
    "        word = re.sub(r'(\\w)\\1{2,}', r'\\1', word)\n",
    "        changed.append(word)\n",
    "    return changed\n",
    "\n",
    "WORDS = Counter(get_words(open('big.txt').read()))\n",
    "\n",
    "def correct_array(words):\n",
    "    return [(correction(w) if w not in WORDS else w) for w in words]\n",
    "\n",
    "def correct_text(text):\n",
    "    words = get_words(text)\n",
    "    return correct_array(words)\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    best = max(candidates(word), key=P)\n",
    "    #if best == 'a':\n",
    "        #print(word, '=>', best)\n",
    "    return best\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits(word)) or known(edits_gen(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    \n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    #inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces)\n",
    "  \n",
    "def edits_gen(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits(word) for e2 in edits(e1))\n",
    "\n",
    "## Lemmatizing\n",
    "\n",
    "def lemmatize_array(arr):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in arr]\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return lemmatize_array(get_words(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_GnqAk20H47d"
   },
   "source": [
    "## 1.5. Cleaning tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "fhB7nhDMH8vl"
   },
   "outputs": [],
   "source": [
    "special_characters_regex = '[!\"_$%&/()=_Ë†*Â¡@' ',:;?#]'\n",
    "retweet_regex = '(.*rt @\\w+)+:'\n",
    "space_regex = '\\s+'\n",
    "url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "             '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "emoticon_regex = '&#\\d+;'\n",
    "mention_regex = '@[\\w\\-]+'\n",
    "number_regex = '\\d+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "rYQVczlUQGPN"
   },
   "outputs": [],
   "source": [
    "data_set['clean'] = data_set.apply(lambda row:\n",
    "                        re.sub(space_regex, ' ',\n",
    "                        re.sub(special_characters_regex, '', \n",
    "                        re.sub(number_regex, ' NUMBERHERE ',\n",
    "                        re.sub('\\s*RT MENTIONHERE', ' MENTIONHERE ',\n",
    "                        re.sub(url_regex, ' LINKHERE ',\n",
    "                        re.sub(mention_regex, ' MENTIONHERE ',\n",
    "                        re.sub(retweet_regex, '',\n",
    "                        re.sub(space_regex, ' ',\n",
    "                              row['clean'])))))), flags=re.ASCII)), axis=1)\n",
    "\n",
    "data_set['clean'] = data_set.apply(lambda row: row['clean'].lower(), axis=1)\n",
    "data_set['clean'] = data_set.apply(lambda row: replace_jargon_from_text(row['clean']), axis=1)\n",
    "data_set['clean'] = data_set.apply(lambda row: delete_multiplied_letters(row['clean']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y5xB2vwFRkyX"
   },
   "source": [
    "## 1.6. Word Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TNoqm2rEaER5",
    "outputId": "059c67b3-4f89-49ce-91fc-24540b947e0a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [rt, mentionhere, as, a, woman, you, shouldn, ...\n",
       "1        [rt, mentionhere, boy, dats, cold, tyga, dwn, ...\n",
       "2        [rt, mentionhere, dawg, rt, mentionhere, you, ...\n",
       "3        [rt, mentionhere, mentionhere, she, look, like...\n",
       "4        [rt, mentionhere, the, shit, you, hear, about,...\n",
       "                               ...                        \n",
       "24778    [you, s, a, muthafin, lie, numberhere, mention...\n",
       "24779    [you, ve, gone, and, broke, the, wrong, heart,...\n",
       "24780    [young, buck, wanna, eat, that, nigguh, like, ...\n",
       "24781        [youu, got, wild, bitches, tellin, you, lies]\n",
       "24782    [ruffled, ntac, eileen, dahlia, beautiful, col...\n",
       "Name: clean, Length: 24783, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set['clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "LMABvmnBRkAX"
   },
   "outputs": [],
   "source": [
    "data_set['corrected'] = data_set.apply(lambda row: correct_array(row['clean']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "6HpQbw0DOcj9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [it, mentionhere, as, a, woman, you, shouldn, ...\n",
       "1     [it, mentionhere, boy, days, cold, tea, own, b...\n",
       "2     [it, mentionhere, dawn, it, mentionhere, you, ...\n",
       "3     [it, mentionhere, mentionhere, she, look, like...\n",
       "4     [it, mentionhere, the, shit, you, hear, about,...\n",
       "5     [mentionhere, the, shit, just, blows, me, clai...\n",
       "6     [mentionhere, i, can, not, just, sit, up, and,...\n",
       "7     [numberhere, mentionhere, because, i, m, tired...\n",
       "8     [amp, you, might, not, get, you, bitch, back, ...\n",
       "9     [mentionhere, hobbies, include, fighting, mari...\n",
       "10    [weeks, is, a, bitch, she, curves, everyone, l...\n",
       "11               [marya, gang, bitch, this, gang, land]\n",
       "12    [so, hoes, that, smoke, are, loses, yea, go, o...\n",
       "13    [bad, birches, is, the, only, thing, that, i, ...\n",
       "14                            [bitch, get, up, off, me]\n",
       "15                   [bitch, nigga, miss, me, with, it]\n",
       "16                               [bitch, ply, whatever]\n",
       "17                          [bitch, who, do, you, love]\n",
       "18                [birches, get, cut, off, everyday, b]\n",
       "19                  [black, bottle, amp, a, bad, bitch]\n",
       "20            [broke, bitch, cannot, tell, me, nothing]\n",
       "21                    [cancel, that, bitch, like, nine]\n",
       "22    [cannot, you, see, these, hoes, will not, change]\n",
       "23    [fuck, no, that, bitch, donor, even, suck, dic...\n",
       "24    [got, you, bitch, tip, toying, on, my, hardwoo...\n",
       "25    [her, pussy, lips, like, heaven, doors, number...\n",
       "26                       [he, what, this, hitting, for]\n",
       "27    [i, met, that, pussy, on, ocean, dr, i, gave, ...\n",
       "28    [i, need, a, tipsy, bitch, who, fuck, on, henn...\n",
       "29    [i, spend, my, money, how, i, want, bitch, thi...\n",
       "Name: corrected, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set['corrected'][:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPfeavePQGPS"
   },
   "source": [
    "### 1.6.1. Setup working sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "lmK81KSN8h0b"
   },
   "outputs": [],
   "source": [
    "def get_joined(r='corrected'):\n",
    "    return data_set.apply(lambda row: ' '.join(row[r]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "ZukT4a0nQGPh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        it mentionhere as a woman you shouldn t compla...\n",
       "1        it mentionhere boy days cold tea own bad for c...\n",
       "2        it mentionhere dawn it mentionhere you ever fu...\n",
       "3        it mentionhere mentionhere she look like a granny\n",
       "4        it mentionhere the shit you hear about me migh...\n",
       "                               ...                        \n",
       "24778    you s a muthafin lie numberhere mentionhere me...\n",
       "24779    you ve gone and broke the wrong heart baby and...\n",
       "24780    young buck anna eat that nigh like i amant muc...\n",
       "24781                   you got wild birches tell you lies\n",
       "24782    ruffled near spleen dahlia beautiful color com...\n",
       "Length: 24783, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_joined()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oszfi3TrrKB8"
   },
   "source": [
    "### 1.6.2. Hatred n-gram dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "vWBiO3EgrLmQ"
   },
   "outputs": [],
   "source": [
    "def get_weight(row):\n",
    "    w = [hd['prophate'] for i,hd in hatred_dict.iterrows() if hd['ngram'] in row]\n",
    "    return w if len(w) > 0 else [0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "vWBiO3EgrLmQ"
   },
   "outputs": [],
   "source": [
    "data_set['hate'] = get_joined().apply(get_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "QsVjzWx0QGP0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "766.3710000000002"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set['hate'].apply(lambda row: max(row)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "THJu41sRrRGY"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    24783.000000\n",
       "mean         0.030923\n",
       "std          0.132975\n",
       "min          0.000000\n",
       "25%          0.000000\n",
       "50%          0.000000\n",
       "75%          0.000000\n",
       "max          0.889000\n",
       "Name: hate, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set['hate'].apply(lambda row: max(row)).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0m-Qatsm7cHo"
   },
   "source": [
    "## 1.7. Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "r5lyy-vN7ZDX"
   },
   "outputs": [],
   "source": [
    "data_set['lem'] = data_set.apply(lambda row: lemmatize_array(row['corrected']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "BpmD254bZeSi"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [it, mentionhere, a, a, woman, you, shouldn, t...\n",
       "1        [it, mentionhere, boy, day, cold, tea, own, ba...\n",
       "2        [it, mentionhere, dawn, it, mentionhere, you, ...\n",
       "3        [it, mentionhere, mentionhere, she, look, like...\n",
       "4        [it, mentionhere, the, shit, you, hear, about,...\n",
       "                               ...                        \n",
       "24778    [you, s, a, muthafin, lie, numberhere, mention...\n",
       "24779    [you, ve, gone, and, broke, the, wrong, heart,...\n",
       "24780    [young, buck, anna, eat, that, nigh, like, i, ...\n",
       "24781              [you, got, wild, birch, tell, you, lie]\n",
       "24782    [ruffled, near, spleen, dahlia, beautiful, col...\n",
       "Name: lem, Length: 24783, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set['lem']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1v3o8k_LcJi"
   },
   "source": [
    "## 1.8. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "-AOni-T8LEBq"
   },
   "outputs": [],
   "source": [
    "sentences = data_set.apply(lambda row: sent_tokenize(' '.join(row['lem'])),axis=1)\n",
    "words = data_set.apply(lambda row: word_tokenize(' '.join(row['lem'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "BKKo7WxjOGI-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [mentionhere, woman, complain, cleaning, house...\n",
       "1        [mentionhere, boy, day, cold, tea, bad, coffin...\n",
       "2        [mentionhere, dawn, mentionhere, ever, fuck, b...\n",
       "3           [mentionhere, mentionhere, look, like, granny]\n",
       "4        [mentionhere, shit, hear, might, true, might, ...\n",
       "                               ...                        \n",
       "24778    [muthafin, lie, numberhere, mentionhere, menti...\n",
       "24779    [gone, broke, wrong, heart, baby, drove, redne...\n",
       "24780    [young, buck, anna, eat, nigh, like, amant, mu...\n",
       "24781                        [got, wild, birch, tell, lie]\n",
       "24782    [ruffled, near, spleen, dahlia, beautiful, col...\n",
       "Name: tokens, Length: 24783, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords += [',', '.', ';']\n",
    "data_set['tokens'] = words.apply(lambda row: [w for w in row if w not in stopwords]) \n",
    "data_set['tokens']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lySCoviQHBvv"
   },
   "source": [
    "# 2. Vectorization\n",
    "\n",
    "## 2.1. TFiDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "PMVh1GlfFXbI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24783, 11920)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=1)\n",
    "X_tf = vectorizer.fit_transform(get_joined('tokens'))\n",
    "X_tf.column = vectorizer.get_feature_names()\n",
    "X_tf.toarray()[0]\n",
    "#X.column\n",
    "X_tf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Sbad0IWHGGP"
   },
   "source": [
    "## 2.2. TFiDF + N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "AZwZLemXDJJk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24783, 104400)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(2,2), min_df=1)\n",
    "X_ngram = vectorizer.fit_transform(get_joined('tokens'))\n",
    "X_ngram.column = vectorizer.get_feature_names()\n",
    "X_ngram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "e_da_pPHQGPn"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[2.9893232 ],\n",
       "        [2.97648589],\n",
       "        [2.96771896],\n",
       "        ...,\n",
       "        [2.82163766],\n",
       "        [1.99845111],\n",
       "        [3.60304159]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ngram.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pnlnj_DSHIsT"
   },
   "source": [
    "## 2.3. TFiDF + N-grams + POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "8MdhGYTFfAtg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [(mentionhere, RB), (woman, NN), (complain, VB...\n",
       "1        [(mentionhere, RB), (boy, JJ), (day, NN), (col...\n",
       "2        [(mentionhere, RB), (dawn, NN), (mentionhere, ...\n",
       "3        [(mentionhere, RB), (mentionhere, RB), (look, ...\n",
       "4        [(mentionhere, RB), (shit, VBN), (hear, JJ), (...\n",
       "                               ...                        \n",
       "24778    [(muthafin, NN), (lie, NN), (numberhere, RB), ...\n",
       "24779    [(gone, VBN), (broke, VBD), (wrong, JJ), (hear...\n",
       "24780    [(young, JJ), (buck, NN), (anna, JJ), (eat, NN...\n",
       "24781    [(got, VBD), (wild, JJ), (birch, NN), (tell, N...\n",
       "24782    [(ruffled, VBN), (near, IN), (spleen, JJ), (da...\n",
       "Name: tokens, Length: 24783, dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged = data_set['tokens'].apply(nltk.pos_tag)\n",
    "tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8C9Ap1rsHNa6"
   },
   "source": [
    "## 2.4. Other Features\n",
    "\n",
    "#### RTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "ayPzlxtRJWqS"
   },
   "outputs": [],
   "source": [
    "data_set['RT'] = df.apply(lambda row: row[\"tweet\"].count(\"RT\") , axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CsEHbtXIMzVI"
   },
   "source": [
    "#### Number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "R7JA26kaMykj"
   },
   "outputs": [],
   "source": [
    "data_set['num_words'] = words.apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v7bU2ZJSNXHg"
   },
   "source": [
    "#### Number of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "7bay_BUQNZdP"
   },
   "outputs": [],
   "source": [
    "data_set['num_sents'] = sentences.apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set['class'] = df['class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Export data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean</th>\n",
       "      <th>corrected</th>\n",
       "      <th>hate</th>\n",
       "      <th>lem</th>\n",
       "      <th>tokens</th>\n",
       "      <th>RT</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_sents</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[rt, mentionhere, as, a, woman, you, shouldn, ...</td>\n",
       "      <td>[it, mentionhere, as, a, woman, you, shouldn, ...</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>[it, mentionhere, a, a, woman, you, shouldn, t...</td>\n",
       "      <td>[mentionhere, woman, complain, cleaning, house...</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[rt, mentionhere, boy, dats, cold, tyga, dwn, ...</td>\n",
       "      <td>[it, mentionhere, boy, days, cold, tea, own, b...</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>[it, mentionhere, boy, day, cold, tea, own, ba...</td>\n",
       "      <td>[mentionhere, boy, day, cold, tea, bad, coffin...</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[rt, mentionhere, dawg, rt, mentionhere, you, ...</td>\n",
       "      <td>[it, mentionhere, dawn, it, mentionhere, you, ...</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>[it, mentionhere, dawn, it, mentionhere, you, ...</td>\n",
       "      <td>[mentionhere, dawn, mentionhere, ever, fuck, b...</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[rt, mentionhere, mentionhere, she, look, like...</td>\n",
       "      <td>[it, mentionhere, mentionhere, she, look, like...</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>[it, mentionhere, mentionhere, she, look, like...</td>\n",
       "      <td>[mentionhere, mentionhere, look, like, granny]</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[rt, mentionhere, the, shit, you, hear, about,...</td>\n",
       "      <td>[it, mentionhere, the, shit, you, hear, about,...</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>[it, mentionhere, the, shit, you, hear, about,...</td>\n",
       "      <td>[mentionhere, shit, hear, might, true, might, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24778</th>\n",
       "      <td>[you, s, a, muthafin, lie, numberhere, mention...</td>\n",
       "      <td>[you, s, a, muthafin, lie, numberhere, mention...</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>[you, s, a, muthafin, lie, numberhere, mention...</td>\n",
       "      <td>[muthafin, lie, numberhere, mentionhere, menti...</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24779</th>\n",
       "      <td>[you, ve, gone, and, broke, the, wrong, heart,...</td>\n",
       "      <td>[you, ve, gone, and, broke, the, wrong, heart,...</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>[you, ve, gone, and, broke, the, wrong, heart,...</td>\n",
       "      <td>[gone, broke, wrong, heart, baby, drove, redne...</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24780</th>\n",
       "      <td>[young, buck, wanna, eat, that, nigguh, like, ...</td>\n",
       "      <td>[young, buck, anna, eat, that, nigh, like, i, ...</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>[young, buck, anna, eat, that, nigh, like, i, ...</td>\n",
       "      <td>[young, buck, anna, eat, nigh, like, amant, mu...</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24781</th>\n",
       "      <td>[youu, got, wild, bitches, tellin, you, lies]</td>\n",
       "      <td>[you, got, wild, birches, tell, you, lies]</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>[you, got, wild, birch, tell, you, lie]</td>\n",
       "      <td>[got, wild, birch, tell, lie]</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24782</th>\n",
       "      <td>[ruffled, ntac, eileen, dahlia, beautiful, col...</td>\n",
       "      <td>[ruffled, near, spleen, dahlia, beautiful, col...</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>[ruffled, near, spleen, dahlia, beautiful, col...</td>\n",
       "      <td>[ruffled, near, spleen, dahlia, beautiful, col...</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24783 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   clean  \\\n",
       "0      [rt, mentionhere, as, a, woman, you, shouldn, ...   \n",
       "1      [rt, mentionhere, boy, dats, cold, tyga, dwn, ...   \n",
       "2      [rt, mentionhere, dawg, rt, mentionhere, you, ...   \n",
       "3      [rt, mentionhere, mentionhere, she, look, like...   \n",
       "4      [rt, mentionhere, the, shit, you, hear, about,...   \n",
       "...                                                  ...   \n",
       "24778  [you, s, a, muthafin, lie, numberhere, mention...   \n",
       "24779  [you, ve, gone, and, broke, the, wrong, heart,...   \n",
       "24780  [young, buck, wanna, eat, that, nigguh, like, ...   \n",
       "24781      [youu, got, wild, bitches, tellin, you, lies]   \n",
       "24782  [ruffled, ntac, eileen, dahlia, beautiful, col...   \n",
       "\n",
       "                                               corrected   hate  \\\n",
       "0      [it, mentionhere, as, a, woman, you, shouldn, ...  [0.0]   \n",
       "1      [it, mentionhere, boy, days, cold, tea, own, b...  [0.0]   \n",
       "2      [it, mentionhere, dawn, it, mentionhere, you, ...  [0.0]   \n",
       "3      [it, mentionhere, mentionhere, she, look, like...  [0.0]   \n",
       "4      [it, mentionhere, the, shit, you, hear, about,...  [0.0]   \n",
       "...                                                  ...    ...   \n",
       "24778  [you, s, a, muthafin, lie, numberhere, mention...  [0.0]   \n",
       "24779  [you, ve, gone, and, broke, the, wrong, heart,...  [0.0]   \n",
       "24780  [young, buck, anna, eat, that, nigh, like, i, ...  [0.0]   \n",
       "24781         [you, got, wild, birches, tell, you, lies]  [0.0]   \n",
       "24782  [ruffled, near, spleen, dahlia, beautiful, col...  [0.0]   \n",
       "\n",
       "                                                     lem  \\\n",
       "0      [it, mentionhere, a, a, woman, you, shouldn, t...   \n",
       "1      [it, mentionhere, boy, day, cold, tea, own, ba...   \n",
       "2      [it, mentionhere, dawn, it, mentionhere, you, ...   \n",
       "3      [it, mentionhere, mentionhere, she, look, like...   \n",
       "4      [it, mentionhere, the, shit, you, hear, about,...   \n",
       "...                                                  ...   \n",
       "24778  [you, s, a, muthafin, lie, numberhere, mention...   \n",
       "24779  [you, ve, gone, and, broke, the, wrong, heart,...   \n",
       "24780  [young, buck, anna, eat, that, nigh, like, i, ...   \n",
       "24781            [you, got, wild, birch, tell, you, lie]   \n",
       "24782  [ruffled, near, spleen, dahlia, beautiful, col...   \n",
       "\n",
       "                                                  tokens  RT  num_words  \\\n",
       "0      [mentionhere, woman, complain, cleaning, house...   1         25   \n",
       "1      [mentionhere, boy, day, cold, tea, bad, coffin...   1         17   \n",
       "2      [mentionhere, dawn, mentionhere, ever, fuck, b...   2         20   \n",
       "3         [mentionhere, mentionhere, look, like, granny]   1          8   \n",
       "4      [mentionhere, shit, hear, might, true, might, ...   1         25   \n",
       "...                                                  ...  ..        ...   \n",
       "24778  [muthafin, lie, numberhere, mentionhere, menti...   0         22   \n",
       "24779  [gone, broke, wrong, heart, baby, drove, redne...   0         14   \n",
       "24780  [young, buck, anna, eat, nigh, like, amant, mu...   0         13   \n",
       "24781                      [got, wild, birch, tell, lie]   0          7   \n",
       "24782  [ruffled, near, spleen, dahlia, beautiful, col...   0         16   \n",
       "\n",
       "       num_sents  class  \n",
       "0              1      2  \n",
       "1              1      1  \n",
       "2              1      1  \n",
       "3              1      1  \n",
       "4              1      1  \n",
       "...          ...    ...  \n",
       "24778          1      1  \n",
       "24779          1      2  \n",
       "24780          1      1  \n",
       "24781          1      1  \n",
       "24782          1      2  \n",
       "\n",
       "[24783 rows x 9 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_save(obj, fn):\n",
    "    with open(fn, 'wb') as f:\n",
    "        pickle.dump(obj, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_save(data_set.drop('lem', axis=1), 'nlp_dataset.bin')\n",
    "pickle_save(tagged, 'nlp_pos.bin')\n",
    "pickle_save(X_tf, 'nlp_tf.bin')\n",
    "pickle_save(X_ngram, 'nlp_tf_ngram.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "9WLOQHuQh7mX",
    "3ZdP2WaTjTIm",
    "aZ3loh-2YyXK"
   ],
   "name": "NLP(5).ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
